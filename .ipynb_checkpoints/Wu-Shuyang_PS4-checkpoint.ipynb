{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 4, due February 23 at 11:59am, mid-day, noon.\n",
    "\n",
    "### Before You Start\n",
    "\n",
    "This is a fun but challenging problem set. It will test your python skills, as well as your understanding of the material in class and in the readings. Start early and debug often! Some notes:\n",
    "\n",
    "* Part 1 is meant to be easy, so get through it quickly.\n",
    "* Part 2 (especially 2.1) will be difficult, but it is the lynchpin of this problem set to make sure to do it well and understand what you've done. If you find your gradient descent algorithm is taking more than a few minutes to complete, debug more, compare notes with others, and go to the TA sessions.\n",
    "* Depending on how well you've done 2.1, parts 2.3 and 4.3 will be relatively painless or incredibly painful. \n",
    "* Part 4 (especially 3.3) will be computationally intensive. Don't leave this until the last minute, otherwise your code might be running when the deadline arrives.\n",
    "* Do the extra credit problems last.\n",
    "\n",
    "Name: Shuyang Wu\n",
    "\n",
    "Date: 2/14/2017\n",
    "\n",
    "Section: C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introduction to the assignment\n",
    "\n",
    "As with the last assignment, you will be using the [Boston Housing Prices Data Set](http://archive.ics.uci.edu/ml/datasets/Housing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt  \n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "bdata = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Getting oriented\n",
    "\n",
    "Use different learning rates\n",
    "\n",
    "### 1.1 Use existing libraries\n",
    "\n",
    "Soon, you will write your own gradient descent algorithm, which you will then use to minimize the squared error cost function.  First, however, let's use the canned versions that come with Python, to make sure we understand what we're aiming to achieve.\n",
    "\n",
    "Using the same Boston housing prices dataset, use the [Linear Regression class](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from sklearn or the [OLS class](http://wiki.scipy.org/Cookbook/OLS) from SciPy to explore the relationship between  median housing price and number of rooms per house. Do this by first (a) regressing the housing price on the number of rooms per house, and then (b) regressing the housing price on the number of rooms per house and the (number of rooms per house) squared.  Interpret your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bdata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4d7b097fedea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(bdata.keys())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(bdata.feature_names)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(bdata.target.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDESCR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bdata' is not defined"
     ]
    }
   ],
   "source": [
    "# uncomment the following if you want to see a lengthy description of the dataset\n",
    "#print(bdata.keys())\n",
    "#print(bdata.feature_names)\n",
    "print(bdata.data.shape)\n",
    "#print(bdata.target.shape)\n",
    "print(bdata.DESCR)\n",
    "\n",
    "# Your code here\n",
    "df = pd.DataFrame(bdata.data)\n",
    "df.columns = bdata.feature_names[:]\n",
    "\n",
    "house = pd.DataFrame(bdata.target)\n",
    "house.columns = ['MEDV']\n",
    "\n",
    "df = pd.concat([df, house], axis=1)\n",
    "\n",
    "#(a)regressing the housing price on the number of rooms per house\n",
    "lm = smf.ols(formula='MEDV ~ RM', data=df).fit()\n",
    "print(lm.params, '\\n')\n",
    "\n",
    "df['RMsqed'] = df['RM']**2\n",
    "#(b)regressing the housing price on the number of rooms per house and the (number of rooms per house) squared\n",
    "lm = smf.ols(formula='MEDV ~ RM + RMsqed', data=df).fit()\n",
    "print(lm.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your observations here*\n",
    "\n",
    "Regressing median house value on number of rooms: \n",
    "\n",
    "median house value = -34.67 + 9.1 * Number of Rooms. The number of rooms is positively associated with the house prices.\n",
    "\n",
    "Regressing the housing price on the number of rooms per house and the (number of rooms per house) squared: \n",
    "\n",
    "median house value = 66.06 -22.64 * Number of Rooms + 2.47 * Number of Rooms^2. The number of rooms is negatively associated with the house prices, but the squared number of rooms is positively associated with house prices but with a smaller power (2.47 comparing to 9.1 from the previous regression).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Training and testing\n",
    "\n",
    "Chances are, for the above problem you used all of your data to fit the regression line. In some circumstances this is a reasonable thing to do, but often this will result in overfitting. Let's redo the above results the ML way, using careful cross-validation.  Since you are now experts in cross-validation, and have written your own cross-validation algorithm from scratch, you can now take a shortcut and use the libraries that others have built for you.\n",
    "\n",
    "Using the [cross-validation functions](http://scikit-learn.org/stable/modules/cross_validation.html) from scikit-learn, use 250-fold cross-validation to fit regression (a) above, i.e. the linear fit of housing price on number of rooms per house. Each fold of cross-validation will give you one slope coefficient and one intercept coefficient.  Plot the distribution of the 250 slope coefficients using a histogram with 25 bins, then draw a vertical line at the value of the slope coefficient that you estimated in 1.1 using the full dataset.  What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x115562b70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFfBJREFUeJzt3X+w5XV93/Hni18RjKyoYXfkNypkSUVjdKXG1lONKLYC\nk2YQbSNKYzuSGsakjmCb2TttpwE7jjWd0owTRDIjoYiJQqKCFE4zTPhhBATcBTdj+eFarqOCloqb\nhX33j/O9y/F6d8+555x7z7n3+3zMnOF7Pud7vt8339193c/9fL/fzzdVhSRp/Ttg2gVIklaHgS9J\nLWHgS1JLGPiS1BIGviS1hIEvSS0xMPCTXJ5kPsm9fW2vSHJbkruT3Jnk1X2fXZxkR5LtSU5fqcIl\nScszTA//CuAti9o+Cmytql8GtgL/GSDJKcA5wGbgDOCyJJlcuZKkUQ0M/Kq6FXh8UfMeYEOz/Hxg\nZ7N8JnB1VT1dVQ8BO4AtkylVkjSOg0b83geBG5J8DAjwuqb9KOC2vvV2Nm2SpCkb9aTt+4ELq+pY\neuH/qcmVJElaCaP28M+rqgsBquraJH/ctO8Ejulb72ieHe75KUmcxEeSRlBVI50bHbaHn+a1YGeS\nNwAkeRO9sXqA64BzkxyS5ATgpcCd+9poVc38a+vWrVOvYag6gVoDx3TNHE/rbFWNa6nOcQzs4Se5\nCugAL0zyCL2rct4H/GGSA4GfAP+yCfBtSa4BtgG7gQtq3AolSRMxMPCr6l37+OjVSzVW1R8AfzBO\nUZKkyfNO2wE6nc60SxhKZ9oFDGnNHE/rnJi1UCOsnTrHkWmNuCRxtGeSFu5v85hK61oSaoVP2kqS\n1rhRL8vUrLFnL2kAe/iS1BIGviS1hIEvSS1h4EtSSxj4ktQSBv56kTx7Lb4kLcHAl6SWMPAlqSUM\nfElqCQNfklrCwF/DNm06niSk72Ttwvt9vTZtOn56BUuaKmfLXMN6Qb/cY5ixn5ojaXpWdLbMJJcn\nmU9y76L2DyTZnuS+JJf0tV+cZEfz2emjFCVJmrxhZsu8AvivwJ8sNCTpAG8HXl5VTyd5UdO+GTgH\n2EzvAeY3JXmZXXlJmr6BPfyquhV4fFHz+4FLqurpZp3vNe1nAVdX1dNV9RC9h5tvmVy5kqRRjXrS\n9iTgHya5PcktSX6laT8KeLRvvZ1NmyRpykZ9AMpBwBFVdVqS1wCfBU5c7kbm5ub2Lnc6nVY8U1KS\nlqPb7dLtdieyraGu0klyHHB9VZ3avP8icGlV/a/m/Q7gNOB9AFV1SdP+ZWBrVd2xxDYd2h9T/1U6\nRe+kfQZeteNVOtJathrPtE3zWvB54I3Nzk8CDqmq7wPXAe9IckiSE4CXAneOUpgkabIGDukkuQro\nAC9M8giwFfgUcEWS+4BdwLsBqmpbkmuAbcBu4AK78ZI0G7zxag1zSEdqn9UY0pEkrXEGviS1xKiX\nZWrGDB7KkdR29vAlqSUMfElqCQNfklrCwJekljDwJaklDPx1osjem68kaSkGviS1hIEvSS1h4EtS\nSxj4ktQSBr4ktYRz6awTzqUjaRB7+JLUEgMDP8nlSeaT3LvEZ7+XZE+SF/S1XZxkR5LtSU6fdMGS\npNEM08O/AnjL4sYkRwNvBh7ua9sMnANsBs4ALkvvsUySpCkbGPhVdSvw+BIffRz40KK2s4Crq+rp\nqnoI2AFsGbdISdL4RhrDT3Im8GhV3bfoo6OAR/ve72zaJElTtuyrdJIcCnyE3nDOWObm5vYudzod\nOp3OuJtsreEfYi5pLel2u3S73YlsK1WDAyLJccD1VXVqkr8H3AT8GAhwNL2e/BbgfICquqT53peB\nrVV1xxLbrGH2rX3rnR7pHcPhAz943KW1KwlVNdK50WGHdNK8qKr7q2pTVZ1YVScA3wZ+uaq+C1wH\nvCPJIUlOAF4K3DlKYZKkyRrmssyrgL8GTkrySJL3LlqlePaHwTbgGmAb8EXgArvxkjQbhhrSWZEd\nO6QzNod0pPZZjSEdSdIa51w664RX50gaxB6+JLWEgS9JLWHgS1JLGPiS1BIGviS1hIG/ThTZey2+\nJC3FwJekljDwJaklDHxJagkDX5JawsCXpJZwLp11wrl0JA1iD1+SWsLAl6SWGOaJV5cnmU9yb1/b\nR5NsT3JPks8lObzvs4uT7Gg+P32lCpckLc8wPfwrgLcsarsR+KWqeiWwA7gYIMkpwDnAZuAM4LL0\nHsskSZqygYFfVbcCjy9qu6mq9jRvbweObpbPBK6uqqer6iF6Pwy2TK5cSdKoJjGGfz69B5YDHAU8\n2vfZzqZNK8y5dCQNMtZlmUn+LbC7qv50lO/Pzc3tXe50OnQ6nXHKkaR1p9vt0u12J7KtVA2+fjvJ\nccD1VXVqX9t7gPcBb6yqXU3bRUBV1aXN+y8DW6vqjiW2WcPsW/vWOz3SO4YLvfvB1+MHj7u0diWh\nqkb6dX7YIZ00r4UdvhX4EHDmQtg3rgPOTXJIkhOAlwJ3jlKYJGmyBg7pJLkK6AAvTPIIsBX4CHAI\n8JXmIpzbq+qCqtqW5BpgG7AbuMBuvCTNhqGGdFZkxw7pjM0hHal9xhnScS6ddcK5dCQN4tQKktQS\nBr4ktYSBL0ktYeBLUksY+JLUEgb+OuFcOpIGMfAlqSUMfElqCQNfklrCwJekljDwJaklnEtnnXAu\nHUmD2MOXpJYw8CWpJQYGfpLLk8wnubev7YgkNyZ5MMkNSTb0fXZxkh1Jtic5faUKlyQtzzA9/CuA\ntyxquwi4qapOBm4GLgZIcgpwDrAZOAO4LM0jsSRJ0zUw8KvqVuDxRc1nAVc2y1cCZzfLZwJXV9XT\nVfUQsAPYMplSJUnjGHUM/8iqmgeoqseAI5v2o4BH+9bb2bRphTmXjqRBJnXS1msCJWnGjXod/nyS\njVU1n2QT8N2mfSdwTN96RzdtS5qbm9u73Ol06HQ6I5YjSetTt9ul2+1OZFupGtw5T3I8cH1Vvbx5\nfynwg6q6NMmHgSOq6qLmpO1ngNfSG8r5CvCyWmInSZZq1jL0zof3juHCcM7gG7CCx11au5JQVSON\n3w7s4Se5CugAL0zyCLAVuAT4bJLzgYfpXZlDVW1Lcg2wDdgNXGCqS9JsGKqHvyI7toc/Nnv4Uvus\naA9fa4Nz6UgaxKkVJKklDHxJagkDX5JawsCXpJYw8CWpJQz8dcK5dCQNYuBLUksY+JLUEga+JLWE\ngS9JLWHgS1JLOJfOOuFcOpIGsYcvSS1h4EtSSxj4ktQSYwV+kg8muT/JvUk+k+SQJEckuTHJg0lu\nSLJhUsVKkkY3cuAneTHwAeBVVXUqvRPA7wQuAm6qqpOBm4GLJ1GoJGk84w7pHAg8N8lBwKHATuAs\n4Mrm8yuBs8fch4bgXDqSBhk58KvqO8DHgEfoBf0Pq+omYGNVzTfrPAYcOYlCJUnjGWdI5/n0evPH\nAS+m19P/Z/AzF4R7gbgkzYBxbrz6NeBbVfUDgCR/DrwOmE+ysarmk2wCvruvDczNze1d7nQ6dDqd\nMcqRpPWn2+3S7XYnsq1UjdYBT7IFuBx4DbALuAL4KnAs8IOqujTJh4EjquqiJb5fo+5bPUlY+AVq\nYfx+8B23weMurV1JqKqRTtiNHPjNjrcC5wK7gbuB3wKeB1wDHAM8DJxTVU8s8V0Df0wGvtQ+Uwv8\ncRj44+sP/GV8y8CX1rBxAt87bSWpJQx8SWoJA1+SWsLAl6SWMPAlqSUM/HXCuXQkDWLgS1JLGPiS\n1BIGviS1hIEvSS1h4EtSS4wzPbJmyOBJ0yS1nT18SWoJA1+SWsLAl6SWMPAlqSXGCvwkG5J8Nsn2\nJN9I8tokRyS5McmDSW5IsmFSxUqSRjduD/8TwBerajPwCuAB4CLgpqo6GbgZuHjMfWgIzqUjaZBx\nHmJ+OHB3Vb1kUfsDwBuqaj7JJqBbVb+4xPd9xOGYfKat1D7TesThCcD3klyR5K4kn0xyGLCxquYB\nquox4Mgx9iFJmpBxbrw6CHgV8NtV9TdJPk5vOGdx93Gf3cm5ubm9y51Oh06nM0Y5krT+dLtdut3u\nRLY1zpDORuC2qjqxef96eoH/EqDTN6RzSzPGv/j7DumMySEdqX2mMqTTDNs8muSkpulNwDeA64D3\nNG3nAV8YdR+SpMkZuYcPkOQVwB8DBwPfAt4LHAhcAxwDPAycU1VPLPFde/hj6u/hL+Nb9vClNWyc\nHv5YgT8OA398Br7UPtO6SkeStIYY+JLUEga+JLWEgS9JLWHgrxPOpSNpEANfklrCwJekljDwJakl\nDHxJagkDX5JaYpzpkTVDBs+SKant7OFLUksY+JLUEga+JLWEgS9JLWHgS1JLjB34SQ5IcleS65r3\nRyS5McmDSW5IsmH8MjWIc+lIGmQSPfwLgW197y8Cbqqqk4GbgYsnsA9J0pjGCvwkRwNvo/dc2wVn\nAVc2y1cCZ4+zD0nSZIzbw/848CF++sGqG6tqHqCqHgOOHHMfkqQJGPlO2yT/GJivqnuSdPaz6j5v\nAZ2bm9u73Ol06HT2t5n1b9Om45mff3jaZUiaId1ul263O5FtpWq0W/KT/CfgnwNPA4cCzwP+HHg1\n0Kmq+SSbgFuqavMS369R971eJWE/Px+X+sbe9RdO2A6eYiF43KW1KwlVNdIVGiMP6VTVR6rq2Ko6\nETgXuLmqfhO4HnhPs9p5wBdG3YeGl+Y6HUnal5W4Dv8S4M1JHgTe1LyXJE3ZyEM6Y+/YIZ2fMc6Q\nznK+43GX1q6pDOlIktYWA1+SWsLAl6SWMPDXCefSkTSIgS9JLWHgS1JLGPiS1BIGviS1hIEvSS0x\n8myZmi3OoyNpEHv4ktQSBr4ktYSBL0ktYeBLUksY+JLUEgb+OuFcOpIGGTnwkxyd5OYk30hyX5Lf\nadqPSHJjkgeT3JBkw+TKlSSNapyHmG8CNlXVPUl+HvgacBbwXuD7VfXRJB8Gjqiqi5b4vk+8WsSH\nmEsaZFoPMX+squ5plp8EtgNH0wv9K5vVrgTOHnUfkqTJmcgYfpLjgVcCtwMbq2oeej8UgCMnsQ9J\n0njGnlqhGc65Friwqp5Msni8YJ/jB3Nzc3uXO50OnU5n3HIkaV3pdrt0u92JbGvkMXyAJAcBfwF8\nqao+0bRtBzpVNd+M899SVZuX+K5j+IuMM4a/nO943KW1aypj+I1PAdsWwr5xHfCeZvk84Atj7kOS\nNAHjXKXzq8BfAffR62YW8BHgTuAa4BjgYeCcqnpiie/bw1/EHr6kQcbp4Y81pDMOA/9nGfiSBpnm\nkI4kaY0w8Fvn50gy9GvTpuOnXbCkCXFIZ4as1p22y92Hf07S7HBIR5I0kIEvSS1h4EtSSxj4ktQS\nBr4ktcTYk6dpNgy+OkdS29nDH9KmTcd7/bqkNc3r8Ie0/GvknwPsGmFPKz+1gtfhS2vXONfhO6Sz\nYnYxWhhL0spwSEeSWsLAl6SWMPDXiSJ759ORpKWsWOAneWuSB5J8M8mHV2o/kqThrMhVOkkOAL4J\nvAn4DvBV4NyqeqBvnYlcpfPII4/wxBM/80Ct/dq8eTMHH3zwUOt2u106nc6qPZxk1H3cQuiwErNl\nLv9qo40bj+Oxxx5a8rOF4znrrHNy1kKNsHbqnMWrdLYAO6rqYYAkVwNnAQ/s91vL9NRTT/Gyl23m\nOc95ydDf2bVrnoMP3sOTT35vkqVMXRforMiWl3+10fz8c5ofkMPZ3w+IaVkr//jXQp1roUZYO3WO\nY6UC/yjg0b7336b3Q2CinnnmGarCj3507zK+9e/ZtWsrw4fYXPNyfHx4+/shMde8njU/77GVVsOa\nvg7/gAMOYM+ev+Pww98+9Hd27fomu0a5H0ozZdOm45mff3hZ3znggMPYs+fHQ6//R3/06RX/zWOU\n/49Z/I1Iy7PcP/dJ/Zmv1Bj+acBcVb21eX8RUFV1ad863r4pSSMYdQx/pQL/QOBBeidt/w9wJ/DO\nqto+8Z1JkoayIkM6VfVMkn8N3Ejv0s/LDXtJmq6pTZ4mSVpdK36nbZIPJrk/yb1JPpPkkEWfPz/J\nnyX5epLbk5yy0jXto84Lk9zXvH5nH+v8YZIdSe5J8srVrrGpYb91Jjk5yV8n+UmS353RGt/V/Hl/\nPcmtSV4+o3We2dR4d5I7k/zqLNbZt95rkuxO8uurWV/f/gcdzzckeSLJXc3r381inc06nebP/f4k\nt6x2jU0Ng47nv2lqvKtZ5+kkz9/vRqtqxV7Ai4FvAYc07/8H8O5F63wU+P1m+WTgppWsaR91/hJw\nL/BzwIH0hqJOXLTOGcBfNsuvBW6f0TpfBPwK8B+A353RGk8DNjTLb53hY3lY3/LLge2zWGez3gHA\n/wT+Avj1WawTeANw3WrXNkKdG4BvAEc17180i3UuWv+fDJOdqzGXzoHAc5McBBxG787bfqcANwNU\n1YPA8Ul+YRXq6rcZuKOqdlXVM8BfAYt7SWcBfwJQVXcAG5JsXN0yB9dZVd+rqq8BT69ybQuGqfH2\nqvph8/Z2evdtrLZh6uy/hvPngT2rWN+CYf5uAnwAuBb47moW12fYOqd908Uwdb4L+FxV7YTev6lV\nrhGGP54L3gn86aCNrmjgV9V3gI8BjwA7gSeq6qZFq32d5n8kyRbgWODolaxrCfcD/yDJEUkOA94G\nHLNoncU3k+1k9YNqmDqnbbk1/hbwpVWp7KcNVWeSs5NsB64Hzl/lGmGIOpO8GDi7qv470wvUYf/c\n/34zJPqXUxq+HabOk4AXJLklyVeT/OaqV7mMf0dJDqX3m/LnBm10RW+8asaTzgKOA34IXJvkXVV1\nVd9qlwCfSHIXcB9wN/DMSta1WFU9kORS4CvAk9OoYRhroc7l1JjkHwHvBV6/ehX2DFtnVX0e+HyS\n1wP/EXjzDNb5X4D+CQpXPfSHrPNrwLFV9eMkZwCfpxeus1bnQcCrgDcCzwVuS3JbVf3tjNW54O3A\nrVU1cFKxlR7S+TXgW1X1g+bXkj8DXte/QlX936o6v6peVVXnAUfSG/dfVVV1RVW9uqo6wBP0Jn/r\nt5Of/gl7dNO2qoaoc+qGqTHJqcAngTOr6vFVLhFY3rGsqluBE5O8YLXq69v3oDpfDVyd5H8DvwH8\ntyRnrnKZA+usqicXhsmq6kvAwTN6PL8N3FBVP6mq79MbTnnFKpe5nL+f5zLEcM7CRlfyxMMWer32\n59DrdXwa+O1F62wADm6W3wd8eiVr2k+tv9D891hgG3D4os/fxrMnbU9jCicah6mzb72twO/NYo1N\n+w7gtGnUt4w6X9K3/Crg0Vmsc9G6VzCFk7ZDHs+NfctbgIdmtM5fpNezPpDeecf7gFNmrc7msw3A\n94FDh9nmig7pVNWdSa6l9+vIbuAu4JNJ/lXv4/okvZMTVybZQ+/M+L9YyZr243NNb2M3cEFV/ai/\nzqr6YpK3Jflb4P/RG4qYuTqbE8l/AzwP2JPkQnp/WZ+clRqB3wdeAFyWJMDuqpr45HoTqPOfJnk3\n8HfAU8A5U6hxmDr7TfPGmkF1/kaS9zefPwW8YxbrrN5wyg30rpJ5BvhkVW2btTqbdc6m99vIU8Ns\n0BuvJKklfMShJLWEgS9JLWHgS1JLGPiS1BIGviS1hIEvSS1h4EtSSxj4ktQS/x9Gcxu4esWPVwAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11550d9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# your code here\n",
    "kf = KFold(n_splits=250)\n",
    "slopes = []\n",
    "\n",
    "for train, test in kf.split(df):\n",
    "    X_train, X_test = df.iloc[train], df.iloc[test]\n",
    "    lm = smf.ols(formula='MEDV ~ RM', data=X_train).fit()\n",
    "    slopes.append(lm.params.get('RM'))\n",
    "\n",
    "plt.hist(slopes, bins = 25)\n",
    "plt.axvline(x=9.102109, color='r', linestyle='dashed', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your observations here*\n",
    "\n",
    "The slope I calculated with the entire dataset fits in the most frequent bin of slopes calculated from cross-validation. This means the estimated parameter 9.102109 is likely to be representative of the true paramter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Regression lines\n",
    "\n",
    "Create a scatterplot of housing price on rooms per house, and add the two regression lines from 1.1 (or 1.2 if you prefer to do so). Show the linear regression line in red, and the linear+quadratic regression line (which may have curvature) in blue.  Label these two regression lines with the corresponding regression equations (i.e. the slope and intercept of the line).\n",
    "\n",
    "Bonus: Add the 95% confidence bands (i.e.,  the area that has a 95% chance of containing the true regression line) to each of these lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x117e9bd68>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEACAYAAABCl1qQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115516208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here\n",
    "plt.scatter(df['RM'],df['MEDV'], color = 'pink')\n",
    "X_plot = np.arange(df.RM.min(),df.RM.max())\n",
    "plt.plot(X_plot, X_plot*9.102109-34.670621, color = 'r')\n",
    "plt.text(4, 0, r'Y = 9.102109 * X - 34.670621', color = 'r')\n",
    "plt.plot(X_plot, (X_plot*X_plot*2.470124) + (X_plot*-22.643262) + 66.058847, color = 'b')\n",
    "plt.text(5, 50, r'Y = 2.470124 * (X)^2 - 22.643262 * X + 66.058847', color = 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gradient descent: Linear Regression\n",
    "\n",
    "This is where it gets fun!\n",
    "\n",
    "### 2.1 Implement gradient descent with one independent variable (Average rooms per house)\n",
    "\n",
    "Implement the batch gradient descent algorithm that we discussed in class. Use the version you implement to regress the housing price on the number of rooms per house. Experiment with 3-4 different values of the learning rate *R*, and do the following:\n",
    "\n",
    "* Report the values of alpha and beta that minimize the loss function\n",
    "* Report the number of iterations it takes for your algorithm to converge (for each value of *R*)\n",
    "* Report the total running time of your algorithm, in seconds\n",
    "\n",
    "Some skeleton code is provided below, but you should feel free to delete this code and start from scratch if you prefer.\n",
    "\n",
    "* *Hint 1: Don't forget to implement a stopping condition, so that at every iteration you check whether your results have converged. Common approaches to this are to (a) check to see if the loss has stopped decreasing; and (b) check if both your current parameter esimates are close to the estimates from the previous iteration.  In both cases, \"close\" should not be ==0, it should be <=epsilon, where epsilon is something very small (like 0.0001).*\n",
    "* *Hint 2: Some people like to include a MaxIterations parameter in their gradient descent algorithm, to prevent divergence. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When R = 0.01\n",
      "Converged at iteration:  31252\n",
      "Time taken: 0.67 seconds\n",
      "(-33.839026020569712, array([ 8.97137807]))\n",
      "\n",
      " When R = 0.02\n",
      "Converged at iteration:  18507\n",
      "Time taken: 0.6 seconds\n",
      "(-34.254887890404511, array([ 9.03675366]))\n",
      "\n",
      " When R = 0.001\n",
      "Converged at iteration:  122107\n",
      "Time taken: 2.41 seconds\n",
      "(-26.353843249632209, array([ 7.79466939]))\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "bivariate_ols\n",
    "    Gradient Decent to minimize OLS. Used to find co-efficients of bivariate OLS Linear regression\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "xvalues, yvalues : narray\n",
    "    xvalues: independent variable\n",
    "    yvalues: dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "MaxIterations: Int\n",
    "    maximum number of iterations\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "alpha: float\n",
    "    intercept\n",
    "    \n",
    "beta: float\n",
    "    co-efficient\n",
    "\"\"\"\n",
    "def bivariate_ols(xvalues, yvalues, R, MaxIterations):\n",
    "    # initialize the parameters\n",
    "    start_time = time.time()\n",
    "    converged = False\n",
    "    iter = 0\n",
    "    m = len(xvalues) \n",
    "    beta = np.random.random(xvalues.shape[1])\n",
    "\n",
    "    # Iterate Loop\n",
    "    while not converged:\n",
    "        J = xvalues.dot(beta) - yvalues\n",
    "        grad = xvalues.T.dot(J)/m\n",
    "       \n",
    "        # update the theta_temp\n",
    "        temp = beta - R * grad\n",
    "    \n",
    "        if (abs(temp - beta) <= 0.0001).all():\n",
    "            print('Converged at iteration: ', iter)\n",
    "            converged = True\n",
    "    \n",
    "        beta = temp   # update error \n",
    "        iter += 1  # update iter\n",
    "    \n",
    "        if iter == MaxIterations:\n",
    "            print('Max interactions exceeded, did not converge.')\n",
    "            converged = True\n",
    "            \n",
    "    print(\"Time taken: \" + str(round(time.time() - start_time,2)) + \" seconds\")\n",
    "    alpha = beta[-1]\n",
    "    beta = beta[:-1]\n",
    "    return alpha, beta\n",
    "\n",
    "# example function call\n",
    "ones = np.ones(df.shape[0])\n",
    "X = np.concatenate((df,ones[:, None]), axis = 1)\n",
    "X = X[:, (5, -1)]\n",
    "Y = np.array(df['MEDV'])\n",
    "print('When R = 0.01')\n",
    "print(bivariate_ols(X, Y, 0.01, 10000000))\n",
    "\n",
    "print('\\n','When R = 0.02')\n",
    "print(bivariate_ols(X, Y, 0.02, 10000000))\n",
    "\n",
    "print('\\n','When R = 0.001')\n",
    "print(bivariate_ols(X, Y, 0.001, 10000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your observations here*\n",
    "\n",
    "The convergence iteration and running time increased as R value decreases (number of iteration increased from 18507 to 122107 while R decreased from 0.02 to 0.001; running time increased from 0.6 sec to 2.41 sec). Smaller learning rate/step size means the amount of change made in each iteration is smaller, which leads to more iterations to reach the same goal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data normalization (done for you!)\n",
    "\n",
    "Soon, you will implement a version of gradient descent that can use an arbitrary number of independent variables. Before doing this, we want to give you some code in case you want to standardize your features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def standardize(raw_data):\n",
    "    return ((raw_data - np.mean(raw_data, axis = 0)) / np.std(raw_data, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Implement gradient descent with an arbitrary number of independent variables\n",
    "\n",
    "Now that you have a simple version of gradient descent working, create a version of gradient descent that can take more than one independent variable.  Assume all independent variables will be continuous.  Test your algorithm using CRIM and RM as independent variables. Standardize these variables before before inputting them to the gradient descent algorithm. \n",
    "\n",
    "As before,  report and interpret your estimated coefficients, the number of iterations before convergence, and the total running time of your algorithm. Experiment with 2-3 different values of R.\n",
    "\n",
    "* *Hint 1: Be careful to implement this efficiently, otherwise it might take a long time for your code to run. Commands like `np.dot` can be a good friend to you on this problem*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When R = 0.001\n",
      "Converged at iteration:  5394\n",
      "Time taken: 0.12 seconds\n",
      "(22.432929691170823, array([-2.26479013,  5.86754342]))\n",
      "\n",
      " When R = 0.01\n",
      "Converged at iteration:  767\n",
      "Time taken: 0.02 seconds\n",
      "(22.522976301102926, array([-2.25145497,  5.89084176]))\n",
      "\n",
      " When R = 0.1\n",
      "Converged at iteration:  95\n",
      "Time taken: 0.0 seconds\n",
      "(22.531933170458924, array([-2.24912505,  5.893868  ]))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "multivariate_ols\n",
    "    Gradient Decent to minimize OLS. Used to find co-efficients of bivariate OLS Linear regression\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "xvalue_matrix, yvalues : narray\n",
    "    xvalue_matrix: independent variable\n",
    "    yvalues: dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "MaxIterations: Int\n",
    "    maximum number of iterations\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "alpha: float\n",
    "    intercept\n",
    "    \n",
    "beta_array: array[float]\n",
    "    co-efficient\n",
    "\"\"\"\n",
    "\n",
    "def multivariate_ols(xvalue_matrix, yvalues, R, MaxIterations):\n",
    "    start_time = time.time()\n",
    "    converged = False\n",
    "    iter = 0\n",
    "    m = len(xvalue_matrix) \n",
    "    beta = np.random.random(xvalue_matrix.shape[1])\n",
    "\n",
    "    # Iterate Loop\n",
    "    while not converged:\n",
    "        J = xvalue_matrix.dot(beta) - yvalues\n",
    "        grad = xvalue_matrix.T.dot(J)/m\n",
    "       \n",
    "        # update the theta_temp\n",
    "        temp = beta - R * grad\n",
    "\n",
    "        if (abs(temp - beta) <= 0.0001).all():\n",
    "            print('Converged at iteration: ', iter)\n",
    "            converged = True\n",
    "    \n",
    "        beta = temp   # update error \n",
    "        iter += 1  # update iter\n",
    "    \n",
    "        if iter == MaxIterations:\n",
    "            print('Max interactions exceeded, did not converge.')\n",
    "            converged = True\n",
    "            \n",
    "    print(\"Time taken: \" + str(round(time.time() - start_time,2)) + \" seconds\")\n",
    "    alpha = beta[-1]\n",
    "    beta = beta[:-1]\n",
    "    return alpha, beta\n",
    "\n",
    "X = standardize(bdata.data[:,(0,5)])\n",
    "ones = np.ones(X.shape[0])\n",
    "X = np.concatenate((X,ones[:, None]), axis = 1)\n",
    "Y = bdata.target[:]\n",
    "print('When R = 0.001')\n",
    "print(multivariate_ols(X, Y, 0.001, 100000))\n",
    "\n",
    "print('\\n','When R = 0.01')\n",
    "print(multivariate_ols(X, Y, 0.01, 100000))\n",
    "\n",
    "print('\\n','When R = 0.1')\n",
    "print(multivariate_ols(X, Y, 0.1, 100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your observations here*\n",
    "\n",
    "The convergence iteration and running time increased as R value decreases (number of iteration increased from 96 to 5412 while R decreased from 0.1 to 0.001; running time increased from 0.0 sec to 0.22 sec). Smaller learning rate/step size means the amount of change made in each iteration is smaller, which leads to more iterations to reach the same goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Compare standardized vs. non-standardized results\n",
    "\n",
    "Repeat the analysis from 2.3, but this time do not standardize your variables - i.e., use the original data. Use the same three values of R (0.1, 0.01, and 0.001). What do you notice about the running time and convergence properties of your algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When R = 0.001\n",
      "Converged at iteration:  110273\n",
      "Time taken: 1.95 seconds\n",
      "(-20.375730710956955, array([-0.29163778,  7.0102108 ])) \n",
      "\n",
      "When R = 0.01\n",
      "Converged at iteration:  31347\n",
      "Time taken: 0.59 seconds\n",
      "(-28.409255132595469, array([-0.26480382,  8.25882586])) \n",
      "\n",
      "When R = 0.1\n",
      "Max interactions exceeded, did not converge.\n",
      "Time taken: 19.61 seconds\n",
      "(nan, array([ nan,  nan])) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "X = bdata.data[:,(0,5)]\n",
    "ones = np.ones(X.shape[0])\n",
    "X = np.concatenate((X,ones[:, None]), axis = 1)\n",
    "Y = bdata.target[:]\n",
    "print('When R = 0.001')\n",
    "print(multivariate_ols(X, Y, 0.001, 1000000), '\\n')\n",
    "\n",
    "print('When R = 0.01')\n",
    "print(multivariate_ols(X, Y, 0.01, 1000000), '\\n')\n",
    "\n",
    "print('When R = 0.1')\n",
    "print(multivariate_ols(X, Y, 0.1, 1000000), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your observations here*\n",
    "\n",
    "Without normalization, the running time became much longer (e.g. when R = 0.001, running time with normalization versus without is 0.11 vs 1.86) and convergence took more iterations to be reached (e.g. when R = 0.001, iteration of convergence with normalization versus without is 5384th vs 108591th)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prediction\n",
    "\n",
    "Let's use our fitted model to make predictions about housing prices. Since the focus is now on prediction rather than the interpretation of the coefficients, it would be wise for you to first standardize your features before proceeding.\n",
    "\n",
    "### 3.1 Cross-Validation\n",
    "\n",
    "Unless you were careful above, you probably overfit your data again. Let's fix that in one of two ways. If you're feeling confident, use k-fold cross-validation to re-fit the multivariate regression from 2.3 above, and report your estimated coefficients (there should be three, corresponding to the intercept and the two coefficients for CRIM and RM). Or if you want to do the quick and dirty version, randomly divide your data into a training set (66%) and testing set (34%) and use the training set to re-fit the regression from 2.3 above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at iteration:  5370\n",
      "Time taken: 0.12 seconds\n",
      "(22.459925368517752, array([-2.44824656,  5.90566875]))\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "std_bdata = standardize(bdata.data)\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(std_bdata, bdata.target, test_size = 0.34, random_state=42)\n",
    "X = Xtrain[:,(0,5)]\n",
    "ones = np.ones(X.shape[0])\n",
    "X = np.concatenate((X,ones[:, None]), axis = 1)\n",
    "Y = Ytrain[:]\n",
    "print(multivariate_ols(X, Y, 0.001, 100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discuss your results here*\n",
    "\n",
    "Training on 66% of the dataset produced similar results comparing to using the whole dataset. \n",
    "\n",
    "Using the entire dataset:  (When R = 0.001)\n",
    "\n",
    "(22.432993121063181, array([-2.26143419,  5.8706921 ]))\n",
    "\n",
    "Using just the taining set:\n",
    "\n",
    "(22.459998372857232, array([-2.45425446,  5.90172244]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Predicted values and RMSE\n",
    "\n",
    "Let's figure out how accurate this predictive model turned out to be. Compute the RMSE on your test cases, i.e. take the model parameters that you found above and compare the actual to the predicted values for just the test instances. If you did this the k-fold way above, this will be the average RMSE across the k test sets. If you did this the quick and dirty way above, this will just be the RMSE on your single test set.\n",
    "\n",
    "What is your test RMSE?  How does it compare to the performance of your nearest neighbor algorithm from the last problem set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.7700287296345092"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "pred = -2.44089124*Xtest[:,0] + 5.91048075*Xtest[:,5] + 22.45981643\n",
    "np.sqrt(((pred - Ytest) ** 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discuss your results here*\n",
    "\n",
    "The test RMSE is 5.77, better (smaller) than the performance of the nearest neighbor algorithm from the last problem set, which was 7.82 before normalization and 9.693 after normalization. I think gradient descent allowed the model to reach global minimun which was not an option with nearest neighbor algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Regularization \n",
    "\n",
    "### 4.1 Get prepped\n",
    "\n",
    "Step 1: Create new interaction variables between each possible pair of the F_s features. If you originally had *K* features, you should now have *K+K^2/2* features. Standardize all of your features.\n",
    "\n",
    "Step 2: For simplicity, generate a single training and testing set.  Randomly sample 66% of your data and call this the training set, and set aside the remaining 34% as your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Step 1\n",
    "features = pd.DataFrame(bdata.data)\n",
    "features.columns = bdata.feature_names[:]\n",
    "outcome = pd.DataFrame(bdata.target)\n",
    "outcome.columns = ['MEDV']\n",
    "\n",
    "for i in features:\n",
    "    for j in features:\n",
    "        if i != j:\n",
    "            outcome[i+'_'+j] = df[i] * df[j]\n",
    "\n",
    "big = pd.concat([features, outcome], axis=1)\n",
    "big_std = standardize(big.ix[:, big.columns != 'MEDV'])\n",
    "big_std['MEDV'] = big['MEDV']\n",
    "\n",
    "##Step 2\n",
    "big_train, big_test = train_test_split(big_std, test_size = 0.34, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Let's overfit!\n",
    "Now, using your version of multivariate regression from 2.3, let's overfit the training data. Using your training set, regress housing price on as many of those *K+K/2* features as you can.  If you get too greedy, it's possible this will take a long time to compute, so start with 5-10 features, and if you have the time, add more features.\n",
    "\n",
    "Report the RMSE when you apply your model to your training set and to your testing set. How do these numbers compare to each other, and to the RMSE from 3.2 and nearest neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressed on first 5 features\n",
      "Converged at iteration:  5440\n",
      "Time taken: 0.96 seconds\n",
      "alpha =  22.8263953739 \n",
      "beta =  [-2.01161109  1.02518485 -2.14004196  2.31413609 -1.21866557]\n",
      "RMSE =  10.2597046662 \n",
      "\n",
      "Regressed on first 10 features\n",
      "Converged at iteration:  10651\n",
      "Time taken: 2.59 seconds\n",
      "alpha =  22.5869767362 \n",
      "beta =  [-1.35352758  1.16007025 -0.34531576  1.37152017 -1.35487774  4.92276364\n",
      " -1.65048231 -3.02607223 -0.10377917 -1.21078612]\n",
      "RMSE =  11.1375455495 \n",
      "\n",
      "Regressed on first 20 features\n",
      "Converged at iteration:  12541\n",
      "Time taken: 3.39 seconds\n",
      "alpha =  22.4100948987 \n",
      "beta =  [-0.03678582  0.48038652  0.43133883  0.06084733 -1.961301    2.94305199\n",
      " -0.59502883 -2.84117856  1.13893873 -0.77553548 -1.855002    0.92233092\n",
      " -3.66868561  0.44713247  0.91939807  1.23991895 -0.24356115 -1.13370055\n",
      "  0.33832398 -0.85826012]\n",
      "RMSE =  11.5513912653 \n",
      "\n",
      "Regressed on first 100 features\n",
      "Converged at iteration:  11283\n",
      "Time taken: 3.82 seconds\n",
      "alpha =  22.34616743 \n",
      "beta =  [  1.90786112e-01  -3.21911231e-01   8.54362927e-01   6.53727276e-02\n",
      "  -1.08657434e-02   3.09649190e+00   3.70689211e-01  -4.95490987e-01\n",
      "   1.54844686e+00  -2.95797165e-02  -1.09545964e+00  -7.88424936e-02\n",
      "   7.18523700e-02   5.95508392e-01   1.76877678e-02   1.05868147e+00\n",
      "  -2.93797851e-01  -4.31728078e-01   5.06069637e-01   2.58742314e-02\n",
      "  -3.22756462e-01   3.09984575e-01   7.12280288e-01   1.39428513e-01\n",
      "  -1.54116923e-01   1.88446709e-01  -4.26897730e-01  -2.13769137e-01\n",
      "  -2.11071251e-01   3.49296895e-01  -5.45009841e-02   2.75871432e-02\n",
      "  -1.73921552e-01   2.23216577e-02   4.22650442e-01   1.08834862e-01\n",
      "  -6.69935201e-01  -4.62892981e-02  -3.18689205e-01  -3.45079781e-01\n",
      "   6.90703398e-02   2.20795539e-01   3.05229400e-01  -4.13471267e-01\n",
      "   6.41635728e-01   4.17195296e-01  -2.49906445e-02   4.66186897e-01\n",
      "  -2.07089095e-01   1.19202305e+00   3.85709807e-01   2.23882020e-02\n",
      "  -6.98356287e-01   1.53176835e-01   5.90521747e-01  -2.05582126e-01\n",
      "  -9.14033651e-01  -3.07905960e-01   3.67888281e-01   1.01383719e+00\n",
      "  -1.19867635e+00  -3.57631178e-01   1.46140487e-01   5.31769514e-01\n",
      "  -2.69856674e-01  -2.58228127e-01  -4.31121690e-01  -1.24021827e+00\n",
      "   3.93686914e-01  -8.26506508e-01  -1.23476612e+00  -6.60514153e-01\n",
      "  -4.51656484e-01  -8.99488976e-01   5.72568179e-01  -3.56940588e-01\n",
      "   4.10570868e-01  -9.34145696e-02   1.48184255e-01   9.47596145e-01\n",
      "  -4.41938763e-01  -1.57764532e+00  -2.27527953e-01   1.77039044e+00\n",
      "  -2.55576055e+00  -1.89507987e-01   8.15411281e-03   1.89340799e-01\n",
      "   2.41626614e-03   1.50581358e-02  -1.38696865e-01  -3.76760573e-01\n",
      "   7.21156288e-01  -1.26681917e-01  -2.06412932e-02  -3.81252917e-01\n",
      "  -7.00799256e-01  -7.22394273e-01  -2.77204151e-01  -7.20680821e-01]\n",
      "RMSE =  11.6647755937 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#first 5 features\n",
    "X = big_train.ix[:, 0:5]\n",
    "ones = np.ones(big_train.shape[0])\n",
    "X = np.concatenate((X,ones[:, None]), axis = 1)\n",
    "Y = big_train['MEDV']\n",
    "print('Regressed on first 5 features')\n",
    "a, b = multivariate_ols(X, Y, 0.001, 100000)\n",
    "print('alpha = ', a, '\\nbeta = ', b)\n",
    "pred = (np.matrix(big_test.ix[:,0:5])) * (np.matrix(b)).T\n",
    "pred = np.array(pred) + a\n",
    "RMSE5 = np.sqrt(((pred - np.array(big_test['MEDV'])) ** 2).mean())\n",
    "print('RMSE = ', RMSE5, '\\n')\n",
    "\n",
    "#first 10 features\n",
    "X = big_train.ix[:, 0:10]\n",
    "ones = np.ones(big_train.shape[0])\n",
    "X = np.concatenate((X,ones[:, None]), axis = 1)\n",
    "Y = big_train['MEDV']\n",
    "print('Regressed on first 10 features')\n",
    "a, b = multivariate_ols(X, Y, 0.001, 100000)\n",
    "print('alpha = ', a, '\\nbeta = ', b)\n",
    "pred = (np.matrix(big_test.ix[:,0:10])) * (np.matrix(b)).T\n",
    "pred = np.array(pred) + a\n",
    "RMSE10 = np.sqrt(((pred - np.array(big_test['MEDV'])) ** 2).mean())\n",
    "print('RMSE = ', RMSE10, '\\n')\n",
    "\n",
    "#first 20 features\n",
    "X = big_train.ix[:, 0:20]\n",
    "ones = np.ones(big_train.shape[0])\n",
    "X = np.concatenate((X,ones[:, None]), axis = 1)\n",
    "Y = big_train['MEDV']\n",
    "print('Regressed on first 20 features')\n",
    "a, b = multivariate_ols(X, Y, 0.001, 100000)\n",
    "print('alpha = ', a, '\\nbeta = ', b)\n",
    "pred = (np.matrix(big_test.ix[:,0:20])) * (np.matrix(b)).T\n",
    "pred = np.array(pred) + a\n",
    "RMSE20 = np.sqrt(((pred - np.array(big_test['MEDV'])) ** 2).mean())\n",
    "print('RMSE = ', RMSE20, '\\n')\n",
    "\n",
    "#first 100 features\n",
    "X = big_train.ix[:, 0:100]\n",
    "ones = np.ones(big_train.shape[0])\n",
    "X = np.concatenate((X,ones[:, None]), axis = 1)\n",
    "Y = big_train['MEDV']\n",
    "print('Regressed on first 100 features')\n",
    "a, b = multivariate_ols(X, Y, 0.001, 100000)\n",
    "print('alpha = ', a, '\\nbeta = ', b)\n",
    "pred = (np.matrix(big_test.ix[:,0:100])) * (np.matrix(b)).T\n",
    "pred = np.array(pred) + a\n",
    "RMSE100 = np.sqrt(((pred - np.array(big_test['MEDV'])) ** 2).mean())\n",
    "print('RMSE = ', RMSE100, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discuss your results here*\n",
    "\n",
    "Adding more features (5 to 100) increased the RMSE from 10.26 to 11.74. Comparing to when only using CRIM and RM as features, the RMSE increased by a large extent (from 5.77 to 10.26 and 11.74). Comparing to the nearest neighbor model which also only used CRIM and RM as features, the RMSE increased from 7.82 {before normalization) and 9.693 (after normalization) to 10.26 through 11.74. I think the increase in caused by introducing little important features which increased the dimensionality but not the prediction accuracy. Thus, the more features included, the higher the RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Ridge regularization\n",
    "Incorporate L2 (Ridge) regularization into your multivariate_ols regression. Write a new version of your gradient descent algorithm that includes a regularization term \"lambda\" to penalize excessive complexity. \n",
    "\n",
    "Use your regularized regression to re-fit the model from 3.2 above on your training data. Try this for several different values of lambda, and report your RMSE for each lambda separately for your training and testing data. How do these numbers compare to each other, to the RMSE from 4.2,  to the RMSE from 2.3, and to the RMSE from nearest neighbors?\n",
    "\n",
    "Go brag to your friends about how you just implemented ridge-regularized multivariate regression using gradient descent optimization, from scratch. If you still have friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When lambda = 0.1\n",
      "Converged at iteration:  5359\n",
      "Time taken: 0.27 seconds\n",
      "(22.453340567994612, array([-2.45432801,  5.90086092]))\n",
      "RMSE =  5.77101194067\n",
      "\n",
      " When lambda = 1\n",
      "Converged at iteration:  5332\n",
      "Time taken: 0.19 seconds\n",
      "(22.393675029923983, array([-2.44266854,  5.89335013]))\n",
      "RMSE =  5.77168549892\n",
      "\n",
      " When lambda = 0.01\n",
      "Converged at iteration:  5389\n",
      "Time taken: 0.14 seconds\n",
      "(22.459269926028377, array([-2.44180617,  5.90845078]))\n",
      "RMSE =  5.77029645113\n",
      "\n",
      " When lambda = 10\n",
      "Converged at iteration:  5203\n",
      "Time taken: 0.11 seconds\n",
      "(21.813777313140459, array([-2.40647596,  5.7917622 ]))\n",
      "RMSE =  5.8017490785\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "def multivariate_ols_ridge(xvalue_matrix, yvalues, lamda, R, MaxIterations=100000):\n",
    "    start_time = time.time()\n",
    "    converged = False\n",
    "    iter = 0\n",
    "    m = len(xvalue_matrix) \n",
    "    beta = np.random.random(xvalue_matrix.shape[1])\n",
    "\n",
    "    # Iterate Loop\n",
    "    while not converged:\n",
    "        J = xvalue_matrix.dot(beta) - yvalues\n",
    "        grad = xvalue_matrix.T.dot(J) + lamda * beta\n",
    "       \n",
    "        # update the theta_temp\n",
    "        temp = beta - R * grad/m\n",
    "\n",
    "        if (abs(temp - beta) <= 0.0001).all():\n",
    "            print('Converged at iteration: ', iter)\n",
    "            converged = True\n",
    "    \n",
    "        beta = temp   # update error \n",
    "        iter += 1  # update iter\n",
    "    \n",
    "        if iter == MaxIterations:\n",
    "            print('Max interactions exceeded, did not converge.')\n",
    "            converged = True\n",
    "            \n",
    "    print(\"Time taken: \" + str(round(time.time() - start_time,2)) + \" seconds\")\n",
    "    alpha = beta[-1]\n",
    "    beta = beta[:-1]\n",
    "    return alpha, beta\n",
    "\n",
    "\n",
    "std_bdata = standardize(bdata.data)\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(std_bdata, bdata.target, test_size = 0.34, random_state=42)\n",
    "X = Xtrain[:,(0,5)]\n",
    "ones = np.ones(X.shape[0])\n",
    "X = np.concatenate((X,ones[:, None]), axis = 1)\n",
    "Y = Ytrain[:]\n",
    "print('When lambda = 0.1')\n",
    "print(multivariate_ols_ridge(X, Y, 0.1, 0.001, 100000))\n",
    "pred = -2.45200156*Xtest[:,0] + 5.90295582*Xtest[:,5] + 22.453244444084891\n",
    "print('RMSE = ', np.sqrt(((pred - Ytest) ** 2).mean()))\n",
    "\n",
    "print(\"\\n\", 'When lambda = 1')\n",
    "print(multivariate_ols_ridge(X, Y, 1, 0.001, 100000))\n",
    "pred = -2.45651429*Xtest[:,0] + 5.88695804*Xtest[:,5] + 22.393691682381704\n",
    "print('RMSE = ', np.sqrt(((pred - Ytest) ** 2).mean()))\n",
    "\n",
    "print(\"\\n\", 'When lambda = 0.01')\n",
    "print(multivariate_ols_ridge(X, Y, 0.01, 0.001, 100000))\n",
    "pred = -2.44387942*Xtest[:,0] + 5.90891932*Xtest[:,5] + 22.459177766203517\n",
    "print('RMSE = ', np.sqrt(((pred - Ytest) ** 2).mean()))\n",
    "\n",
    "print(\"\\n\", 'When lambda = 10')\n",
    "print(multivariate_ols_ridge(X, Y, 10, 0.001, 100000))\n",
    "pred = -2.40869855*Xtest[:,0] + 5.79118048*Xtest[:,5] + 21.813659904385176\n",
    "print('RMSE = ', np.sqrt(((pred - Ytest) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discuss your results here*\n",
    "\n",
    "I changed the lambda value from 0.01, 0.1, 1 and 10, the changes in RMSE were minimal. The smallest RMSE is 5.7702 corresponding to lambda equals to 0.01. Comparing to the RMSE (which is around 11) from 4.2, RMSE is drastically smaller. I think because in 4.2, I included more features that are less important, they increased the dimensionality but also error rates. Comparing to the RMSE from 2.3 which is 5.770, the RMSE after ridge regularization, 5.7702, is actually a little bit bigger. I think it's because there are only two features fitted and they are both relatively important so regularization is not really necessary. Comparing to the RMSE from nearest neighbors, 7.82 before normalization and 9.693 after normalization, using gradient descent decreased the RMSE to 5.77."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
